CPU：第一轮五个参数，

task_def = { # min, max, default, num
    "ap-ml-cpu-hyperparam-d": [0.0, 1.0, 0.3, 4],
    "ap-ml-cpu-hyperparam-wdeltal": [0.0, 1.0, 0.001, 4],
    "ap-ml-cpu-hyperparam-wdeltam": [0.0, 1.0, 0.001,4],
    "ap-ml-cpu-hyperparam-wo": [0.0, 1.0, 0.8, 4],
    "ap-ml-cpu-hyperparam-wu": [0.0, 1.0, 0.005, 4],
    "ap-ml-memory-hyperparam-d": [0.0, 1.0, 0.99, 10],
    "ap-ml-memory-hyperparam-wdeltal": [0.0, 1.0, 0.001, 10],
    "ap-ml-memory-hyperparam-wdeltam": [0.0, 1.0, 0.01, 10],
    "ap-ml-memory-hyperparam-wo": [0.0, 1.0, 80, 10],
    "ap-ml-memory-hyperparam-wu": [0.0, 1.0, 0, 10]
}
task_def = { # min, max, default, num
    "ap-ml-cpu-hyperparam-d": [0.0, 1.0, 0.3, 6],
    "ap-ml-cpu-hyperparam-wdeltal": [0.0, 1.0, 0.001, 6],
    "ap-ml-cpu-hyperparam-wdeltam": [0.0, 1.0, 0.001,6],
    "ap-ml-cpu-hyperparam-wo": [0.0, 1.0, 0.8, 6],
    "ap-ml-cpu-hyperparam-wu": [0.0, 1.0, 0.005, 6],
    "ap-ml-memory-hyperparam-d": [0.0, 1.0, 0.99, 10],
    "ap-ml-memory-hyperparam-wdeltal": [0.0, 1.0, 0.001, 10],
    "ap-ml-memory-hyperparam-wdeltam": [0.0, 1.0, 0.01, 10],
    "ap-ml-memory-hyperparam-wo": [0.0, 1.0, 80, 10],
    "ap-ml-memory-hyperparam-wu": [0.0, 1.0, 0, 10]
}
筛选条件是：    
if result['cpu-overrun-seconds'] > 20000:
continue
if result['cpu-request-adjust-times'] > 600:
continue

4核 1024个任务 4小时
32核  16384个任务   8小时
第二轮 26的3次方 个任务

实际 32核 14654个任务 10:11:26
4核 14654个任务 80:88 = 81:28
4核 1024个任务 =5.78
ps -p 13222 -o etime

内存的实际：5小时33分钟 3139个 （因为有OOM，recommend的次数明显变多）


memory优化：在内存overrun很多的时候，直接返回，不生成文件


memory的问题！！很神奇，average gap很高，trace变高以后就再也不变低了
如图Figure_compare_bestcpumem_1.png所示
怀疑： 和CPU不同，memory的oL和uL都在0到1之间，而cpu的可能高达600
但是最优解中wdl和wdm都已经是0了，问题出在哪里：
不对！wdm不是0，先让wdm是0看看-----

以下三行都没问题！！！
首先检查代码中histogram的更新是否有问题
应该不是recommender代码的问题，Figure_Google_trace_ml那个图，用五个0.6跑出来的数据也是有波动的
检查simulator在构造ML memory的时候有什么问题


wdm=0时：也不行
recommender代码可能有问题：Figure_Google_trace_ml那个图，用五个0.6跑出来的：是因为反复的OOM，触发了OOM规则，才使结果很好的
观察代码，histogram更新没有问题，正常更新
OOM规则是否每次都触发了？




跟rule相比，ml的总是突变，因为ml中含有delta函数，而rule的是基于指数衰减的