Time-slicing is supported by basically every GPU architecture and is the simplest solution for sharing a GPU in a Kubernetes cluster. However, constant switching among processes creates a computation time overhead. Also, time-slicing does not provide any level of memory isolation among the processes sharing a GPU, nor any memory allocation limits, which can lead to frequent Out-Of-Memory (OOM) errors.


kubectl patch clusterpolicies.nvidia.com/cluster-policy \
    -n kube-system --type merge \
    -p '{"spec": {"devicePlugin": {"config": {"name": "time-slicing-config-all", "default": "any"}}}}'


https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html
这个文档是行得通的！！

分成5份gpu，有4份被成功分配，1份一直在pending
time slicing中每个pod可以分给不同数量的slice，可以用于scaling。。
设置的request必须等于limit（或者不设置request）
time slicing不能限制显存的大小。。。
虽然管理方式和memory一样都是虚拟内存。但是普通的memory能限制。
再就是time slicing的额外开销较大


取消time slicing只需要删除对应配置即可