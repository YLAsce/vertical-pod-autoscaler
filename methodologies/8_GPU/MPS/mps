https://github.com/NVIDIA/k8s-device-plugin/issues/443 官方issue讨论

https://towardsdatascience.com/how-to-increase-gpu-utilization-in-kubernetes-with-nvidia-mps-e680d20c3181
使用nebuly的fork nvidia device plugin 

安装：首先要k edit daemonset nvidia-device-plugin-1716883380 -n kube-system  
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: feature.node.kubernetes.io/pci-10de.present
                operator: In
                values:
                - "true"
              - key: nos.nebuly.com/gpu-partitioning
                operator: NotIn
                values:
                - mps
            - matchExpressions:
              - key: feature.node.kubernetes.io/cpu-model.vendor_id
                operator: In
                values:
                - NVIDIA
              - key: nos.nebuly.com/gpu-partitioning
                operator: NotIn
                values:
                - mps
            - matchExpressions:
              - key: nvidia.com/gpu.present
                operator: In
                values:
                - "true"
              - key: nos.nebuly.com/gpu-partitioning
                operator: NotIn
                values:
                - mps

注意affinity之间是或者的关系
然后label节点，然后helm安装。
然后编辑config在：k edit configmap nos-device-plugin-configs -n nebuly-nvidia 

apiVersion: v1
data:
  default: "version: v1\nflags:\n  migStrategy: none\nsharing:\n  mps: \n    resources:\n
    \     - name: nvidia.com/gpu\n        rename: nvidia.com/gpu-4gb\n        memoryGB:
    4\n        replicas: 2\n        devices: [\"0\"]\n"
kind: ConfigMap
metadata:
  annotations:
    meta.helm.sh/release-name: nvidia-device-plugin-1717103781
    meta.helm.sh/release-namespace: nebuly-nvidia
  creationTimestamp: "2024-05-30T21:16:23Z"
  labels:
    app.kubernetes.io/instance: nvidia-device-plugin-1717103781
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nebuly-nvidia-device-plugin
    app.kubernetes.io/version: 0.13.0
    helm.sh/chart: nvidia-device-plugin-0.13.0
  name: nos-device-plugin-configs
  namespace: nebuly-nvidia
  resourceVersion: "17743341450"
  uid: dbd9708e-12e2-4081-b1b2-160c5ee03402


然后要删除k get pods -n nebuly-nvidia中的pod，让它重启，然后就能生效了


内存的隔离是怎么回事？。。需要测试
1.一个pod crash了会影响其他的吗
2.一个pod 显存使用过多会导致其他pod崩溃吗



warp core和block的关系
什么之间是shared memory的？GPU程序可以读自己的memory吗？有没有VM？time slicing中，memory有context switch吗